Alexus Sanepid = 0.70373
https://github.com/AlexeiMoroz/Data-Science-Python-Lybraries/blob/6ac90a9c3840036f1977379f27cab65e847491af/Cource_project/Cource_project.ipynb
Здравствуйте, Алексей.
R2 = 0.70373- хороший результат!
Что хорошо реализовано:
* наличие комментариев и заголовков
* выполнен хороший EDA с последующей трансформацией признаков на основе анализа
* вспомогательный код (отрисовка графиков и т.п.) вынесен в функции
* обработка данных реализована классом - это повышает надежность кода
* созданы новые признаки - это очень хорошая практика для соревнований, особенно, если вы выбираете модели на деревьях
* хороший читаемый код
* выполнены все требования проекта
Что можно улучшить:
* можно было попробовать сравнить несколько алгоритмов и выбрать лучший (или написать текстом, что были попробованы... показали результат... поэтому выбрана данная модель)
* можно подобрать гиперпараметры модели (или также текстом написать, что это лучшие параметры)

Дам несколько общих советов:

* если используете какие-то однотипные преобразования, запишите их как функции, чтобы написать код один раз и переиспользовать его потом сколько нужно
* Чтобы не забыть произвести все необходимые этапы предобработки данных для тестовой части можно объединить train и test в один датафрейм, применить функции предобработки, а за тем заново разделить на два файла (удобно это делать заведя переменную-флаг, например df['is_train']True для тренировочных примеров и df['is_train']False для тестовых). Сделать так мы можем потому, что изначально предполагается, что тренировочные и тестовые данные взяты из одного распределения (если это не так, то сколь угодно хорошая модель будет плохо работать на тестовых данных и это будет не её вина).Но надо помнить, что такие операции как стандартизация, нормализация и масштабирование данных нужно делать раздельно (scaler.fit_transform на train и scaler.transform на test), потому что тестовые данные имитируют работу модели на новых данных, которые ни мы, ни модель раньше не видели. На практике иногда этим пренебрегают, полагая что распределение признаков со временем не меняется, но тогда это надо обосновать)
* полезно создавать отдельные папки под данные, модели и результаты
* чтобы не запутаться в результатах экспериментов, можно добавлять в конец названия моделей (которую можно сохранить в формате pickle) и результатов текущую дату или величину метрики на валидации (или и то и другое)
* модели основанные на деревьях решений можно тестировать на out-of-bag выборке (https://dyakonov.org/2016/11/14/случайный-лес-random-forest/), тем самым можно разделить обучающие данные на train и test, обучить на train модель (считая метрику качества на train и oob), а test использовать уже для окончательной проверки модели (чтобы test никак не участвовала в обучении и валидации модели)
* в целом, в реальной жизни при наличии достаточного объема данных так и делают: разбивают данные на train/validate и оставляют некоторый процент самых свежих данных (если у выборки есть привязка ко времени) для финального теста - так мы можем быть уверены, что модель будет хорошо работать в production и нам не придется переделывать ее через неделю)
* Попробуйте применить на этих данных CatBoost (https://catboost.ai) - это очень крутая реализация градинетного бустинга. Когда в будущем вам встретится задача с большим количеством категориальных признаков (как DistrictId, Ecology_2 и тд в нашем случае) обязательно попробуйте CatBoost - под капотом он хитро кодирует категориальные признаки)
* Параметры не обязательно перебирать вслепую. Полезно строить так называемые "кривые валидации", позволяющие смотреть, как меняется ошибка на train и validation в зависимости от параметров (в том числе это позволяет увидеть момент переобучения): https://habr.com/ru/company/ods/blog/323890/#5-krivye-validacii-i-obucheniya
Спасибо за хорошую работу на курсе.
Успехов в дальнейшем обучении!